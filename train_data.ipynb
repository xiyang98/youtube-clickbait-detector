{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from gensim.parsing.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "\n",
    "    \"\"\" Tokenizes a string.\n",
    "\n",
    "    Adds a space between numbers and letters, removes punctuation, repeated whitespaces, words shorter than 2\n",
    "    characters, and stop-words. Returns a list of stems and, eventually, emojis.\n",
    "\n",
    "    @param string: String to tokenize.\n",
    "    @return: A list of stems and emojis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Based on the Ranks NL (Google) stopwords list, but \"how\" and \"will\" are not stripped, and words shorter than 2\n",
    "    # characters are not checked (since they are stripped):\n",
    "    stop_words = [\n",
    "        \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \"com\", \"for\", \"from\", \"in\", \"is\", \"it\", \"of\", \"on\", \"or\", \"that\",\n",
    "        \"the\", \"this\", \"to\", \"was\", \"what\", \"when\", \"where\", \"who\", \"with\", \"the\", \"www\"\n",
    "    ]\n",
    "\n",
    "    string = strip_short(\n",
    "        strip_multiple_whitespaces(\n",
    "            strip_punctuation(\n",
    "                split_alphanum(string))),\n",
    "        minsize=2)\n",
    "    # Parse emojis:\n",
    "    emojis = [ c for c in string if c in emoji.UNICODE_EMOJI ]\n",
    "    # Remove every non-word character and stem each word:\n",
    "    string = stem_text(re.sub(r\"[^\\w\\s,]\", \"\", string))\n",
    "    # List of stems and emojis:\n",
    "    tokens = string.split() + emojis\n",
    "    \n",
    "    for stop_word in stop_words:\n",
    "        try:\n",
    "            tokens.remove(stop_word)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read in our data\n",
    "clickbait = pd.read_csv('clickbaits.csv',names = ['vid','title','img'],encoding = \"ISO-8859-1\")\n",
    "clickbait['label'] = 1\n",
    "clickbait[\"video_title_tokenized\"] = clickbait[\"title\"].apply(tokenize)\n",
    "nonclickbait = pd.read_csv('non_clickbait.csv',names = ['vid','title','img'],encoding = \"ISO-8859-1\")\n",
    "nonclickbait['label'] = 0\n",
    "nonclickbait[\"video_title_tokenized\"] = nonclickbait[\"title\"].apply(tokenize)\n",
    "\n",
    "\n",
    "# concatenate two dfs together\n",
    "newdf = pd.concat([ clickbait, nonclickbait ]).sample(frac=1).sample(frac=1)\n",
    "\n",
    "# add the other columns and set as null\n",
    "newdf[\"video_views\"] = np.nan\n",
    "newdf[\"video_likes\"] = np.nan\n",
    "newdf[\"video_dislikes\"] = np.nan\n",
    "newdf[\"video_comments\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the log of the video metadata or replace the missing values with the mean values obtained\n",
    "# from the train set:\n",
    "mean_log_video_views = pickle.load(open(\"mean-log-video-views\", \"rb\"))\n",
    "mean_log_video_likes = pickle.load(open(\"mean-log-video-likes\", \"rb\"))\n",
    "mean_log_video_dislikes = pickle.load(open(\"mean-log-video-dislikes\", \"rb\"))\n",
    "mean_log_video_comments = pickle.load(open(\"mean-log-video-comments\", \"rb\"))\n",
    "if newdf[\"video_views\"].isnull().any():\n",
    "    newdf[\"video_views\"].fillna(mean_log_video_views, inplace=True)\n",
    "if newdf[\"video_likes\"].isnull().any():\n",
    "    newdf[\"video_likes\"].fillna(mean_log_video_likes, inplace=True)\n",
    "if newdf[\"video_dislikes\"].isnull().any():\n",
    "    newdf[\"video_dislikes\"].fillna(mean_log_video_dislikes, inplace=True)\n",
    "if newdf[\"video_comments\"].isnull().any():\n",
    "    newdf[\"video_comments\"].fillna(mean_log_video_comments, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_embedding(tokens, word2vec, na_vector=None):\n",
    "\n",
    "    \"\"\" Embeds a title with the average representation of its tokens.\n",
    "\n",
    "    Returns the mean vector representation of the tokens representations. When no token is in the Word2Vec model, it\n",
    "    can be provided a vector to use instead (for example the mean vector representation of the train set titles).\n",
    "\n",
    "    @param tokens: List of tokens to embed.\n",
    "    @param word2vec: Word2Vec model.\n",
    "    @param na_vector: Vector representation to use when no token is in the Word2Vec model.\n",
    "    @return: A vector representation for the token list.\n",
    "    \"\"\"\n",
    "\n",
    "    vectors = list()\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in word2vec:\n",
    "            vectors.append(word2vec[token])\n",
    "\n",
    "    if len(vectors) == 0 and na_vector is not None:\n",
    "        vectors.append(na_vector)\n",
    "\n",
    "    return np.mean(np.array(vectors), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = pickle.load(open(\"word2vec\", \"rb\"))\n",
    "mean_title_embedding = pickle.load(open(\"mean-title-embedding\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/CarrieYang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/Users/CarrieYang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# For the test set use the mean title embedding computed on the train set:\n",
    "titles_embeddings = newdf[\"video_title_tokenized\"].apply(average_embedding, word2vec=word2vec, na_vector=mean_title_embedding)\n",
    "new_test_set = pd.concat(\n",
    "    [\n",
    "        newdf[[\"video_views\", \"video_likes\", \"video_dislikes\", \"video_comments\"]],\n",
    "        titles_embeddings.apply(pd.Series)\n",
    "    ], axis=1)\n",
    "new_test_set[\"label\"] = newdf['label']\n",
    "new_test_set[[\"video_views\", \"video_likes\", \"video_dislikes\", \"video_comments\"]] = new_test_set[[\"video_views\", \"video_likes\", \"video_dislikes\", \"video_comments\"]].apply(np.log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = pickle.load(open(\"min-max-scaler\", \"rb\"))\n",
    "\n",
    "new_test_set = new_test_set.replace(-np.inf, 0)\n",
    "new_test_labels = new_test_set[\"label\"]\n",
    "new_test_set = new_test_set.drop(columns=[\"label\"])\n",
    "new_test_set = pd.DataFrame(min_max_scaler.transform(new_test_set), columns=new_test_set.columns)\n",
    "# Import the SVM model:\n",
    "svm = pickle.load(open(\"svm\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1699, 29)\n"
     ]
    }
   ],
   "source": [
    "print(new_test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = svm.predict(new_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the test set (1699 samples):\n",
      "\tAccuracy Score: 0.42377869334902885\n",
      "\tArea under ROC curve: 0.5012143186987976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"Performance on the test set (%d samples):\" % len(new_test_set))\n",
    "print(\"\\tAccuracy Score:\", accuracy_score(new_test_labels, predictions))\n",
    "print(\"\\tArea under ROC curve:\", roc_auc_score(new_test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
